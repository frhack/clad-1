{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Complex step approximation and Automatic Differentiation\n",
    "### Francesco Pasqualini\n",
    "## Complex step approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd59560",
   "metadata": {},
   "source": [
    "$$  \\begin{align}   \n",
    "\n",
    "{f(x + ih) = f (x) + ih f^\\prime(x) - {{h^2 } \\over 2!} f^{\\prime\\prime}(x) + \\mathcal{O}(h^{3})\\tag{1}  }\n",
    "\n",
    "\\end{align} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c13d2e3",
   "metadata": {},
   "source": [
    "$$  \\begin{align}   \n",
    "\n",
    "{ f(x) = Re f(x + ih) + \\mathcal{O}(h^{2})  } \\tag{2}  \n",
    "\\end{align} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3294c654",
   "metadata": {},
   "source": [
    "$$  \\begin{align}   \n",
    "\n",
    "{ f^\\prime(x) = Im {f(x + ih)\\over h} + \\mathcal{O}(h^{2})  } \\tag{3}  \n",
    "\\end{align} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d7f8a4",
   "metadata": {},
   "source": [
    "$$  \\begin{align}   \n",
    "\n",
    "{ f^\\prime(x) = Im {f(x + ih)\\over h} -  {{h^2 } \\over 2!} f^{\\prime\\prime}(x) + \\mathcal{O}(h^{3})  } \\tag{4}  \n",
    "\\end{align} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ecd4d7",
   "metadata": {},
   "source": [
    "\n",
    ">The usual way to approximate derivatives is with finite differences, for example by the forward difference approximation\n",
    "This approximation has error $\\mathcal{O}(h)$ so it is less accurate than the complex step approximation for a given h, but more importantly it is prone to numerical cancellation. For small $h$, $f(x+h)$ and $f(x)$ agree to many significant digits and so in floating-point arithmetic the difference approximation suffers a loss of significant digits. Consequently, as $h$ decreases the error in the computed approximation eventually starts to increase. As numerical analysis textbooks explain, the optimal choice of h that balances truncation error and rounding errors is approximately $h_{opt}$ \n",
    "\n",
    "\n",
    "$$  \\begin{align}   \n",
    "{ f^\\prime(x) = {{f(x + ih)-f(x)}\\over h} + \\mathcal{O}(h)  }  \\tag{5}\n",
    "\\end{align} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39d32c1",
   "metadata": {},
   "source": [
    "$$  \n",
    "\\begin{align}   \n",
    "\n",
    "{ h_{opt} = 2 { \\bigg\\vert {u f(x) \\over f^{\\prime\\prime}(x)} \\bigg\\vert }}^2 \\tag{6}\n",
    "\\end{align} $$\n",
    ">where $u$ is the unit roundoff. The optimal error is therefore of order $u^{1/2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73809b12",
   "metadata": {},
   "source": [
    "When using the complex-step derivative approximation, in order to effectively\n",
    "eliminate truncation errors, it is typical to use a step that is many orders of\n",
    "magnitude smaller than the real part of the calculation. When the truncation errors are eliminated, the higher-order terms of the derivative approximation (4)\n",
    "are so small that they vanish when added to other terms using finite-precision\n",
    "arithmetic. We obtain:\n",
    "\n",
    "$$  \\begin{align}   \n",
    "\n",
    "{f(x + ih) \\equiv f (x) + ih f^\\prime(x) \\tag{7}  }\n",
    "\n",
    "\\end{align} $$\n",
    "\n",
    "where the imaginary part is exactly the derivative of $f$ times $h$.\n",
    "The end result\n",
    "is a sensitivity calculation method that is equivalent to the forward mode of\n",
    "algorithmic differentiation (also known as automatic differentiation or AD), as observed by Griewank [2000, chap. 10, p. 227].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6070a7fb",
   "metadata": {},
   "source": [
    "## Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8de11f",
   "metadata": {},
   "source": [
    "Just as the complex step approximation is based on complex numbers, so the automatic differentiation is based on double numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b0e413",
   "metadata": {},
   "source": [
    "### Dual numbers\n",
    "Dual numbers are a hypercomplex number system first introduced in the 19th century. They are expressions of the form $a + bε$, where a and b are real numbers, and ε is a symbol taken to satisfy ${\\displaystyle \\varepsilon ^{2}=0}$.\n",
    "\n",
    "$${\\displaystyle f(a+b\\varepsilon )=\\sum _{n=0}^{\\infty }{\\frac {f^{(n)}(a)b^{n}\\varepsilon ^{n}}{n!}}=f(a)+bf'(a)\\varepsilon }$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7374de0",
   "metadata": {},
   "source": [
    "So using Taylor seires we can extend any (analytic) real function to the dual numbers.\n",
    "The interesting thing is that by computing compositions of these extended functions over the dual numbers and examining the coefficient of ε in the result we find we have automatically computed the derivative of the composition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e283dc52",
   "metadata": {},
   "source": [
    "### Automatc differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff81ae5e",
   "metadata": {},
   "source": [
    ">In mathematics and computer algebra, automatic differentiation\n",
    "(AD) is a set of techniques to evaluate the derivative of a function specified\n",
    "by a computer program. AD exploits the fact that every computer program, no\n",
    "matter how complicated, executes a sequence of elementary arithmetic operations (addition, subtraction, multiplication, division, etc.), elementary functions\n",
    "(exp, log, sin, cos, etc.) and control flow statements. AD takes source code of a\n",
    "function as input and produces source code of the derived function. By applying\n",
    "the chain rule repeatedly to these operations, derivatives of arbitrary order can\n",
    "be computed automatically, accurately to working precision, and using at most\n",
    "a small constant factor more arithmetic operations than the original program.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f97fc7d",
   "metadata": {},
   "source": [
    "## References\n",
    "[What Is the Complex Step Approximation? - Nick Higham](https://nhigham.com/2020/10/06/what-is-the-complex-step-approximation/)\n",
    "\n",
    "[The Complex-Step Derivative Approximation - JOAQUIM R. R. A. MARTINS 2003](https://www.researchgate.net/publication/222112601_The_Complex-Step_Derivative_Approximation)\n",
    "\n",
    "[The complex-step derivative approximation - Joaquim J Martins, Peter Sturdza, Juan J Alonso 2017](https://hal.archives-ouvertes.fr/hal-01483287/document)\n",
    "\n",
    "[Automatic Differentiation in ROOT - Vassil Vassilev, Aleksandr Efremov, Oksana Shadura 2019](https://www.epj-conferences.org/articles/epjconf/pdf/2020/21/epjconf_chep2020_02015.pdf)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296e3c1b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f2769df",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa653ec6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f260615",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#include <string>\n",
    "#include <fstream>\n",
    "#include \"xtl/xbase64.hpp\"\n",
    "#include \"nlohmann/json.hpp\"\n",
    "#pragma cling add_include_path(\"/srv/conda/envs/notebook/include/python3.7m\")\n",
    "#pragma cling add_include_path(\"/srv/conda/envs/notebook/lib/python3.7/site-packages/numpy/core/include/\")\n",
    "#pragma cling add_library_path(\"/srv/conda/envs/notebook/lib/\")\n",
    "#pragma cling load(\"python3.7m\")\n",
    "#include \"clad/Differentiator/Differentiator.h\"\n",
    "#include <iostream>\n",
    "#include \"../../matplotlibcpp.h\"\n",
    "#include <iostream>\n",
    "#include <complex>\n",
    "#include <cmath>\n",
    "\n",
    "\n",
    "namespace plt = matplotlibcpp;\n",
    "namespace im\n",
    "{\n",
    "    struct image\n",
    "    {   \n",
    "        inline image(const std::string& filename)\n",
    "        {\n",
    "            std::ifstream fin(filename, std::ios::binary);   \n",
    "            m_buffer << fin.rdbuf();\n",
    "        }        \n",
    "        std::stringstream m_buffer;\n",
    "    };\n",
    "    nl::json mime_bundle_repr(const image& i)\n",
    "    {\n",
    "        auto bundle = nl::json::object();\n",
    "        bundle[\"image/png\"] = xtl::base64encode(i.m_buffer.str());\n",
    "        return bundle;\n",
    "    }\n",
    "}\n",
    "\n",
    "using namespace std;\n",
    "using namespace std::complex_literals;\n",
    "\n",
    "typedef complex<long double> dcomp;\n",
    "\n",
    "\n",
    "long double function_fn(long double x) {\n",
    "  return exp(x)/(pow((cos(x)),3) + pow(sin(x),3));\n",
    "}\n",
    "\n",
    "dcomp function_cfn(dcomp x) {\n",
    "  return exp(x)/(pow((cos(x)),3) + pow(sin(x),3));\n",
    "}\n",
    "\n",
    "\n",
    "double derivative_fn(double x) {\n",
    "  return (exp(x)*(cos(3*x) + sin(3*x)/2 + (3*sin(x))/2)) / \n",
    "            pow(pow(cos(x),3) + pow(sin(x),3),2);\n",
    "}\n",
    "\n",
    "long double derivative_finite_diff_approx(long double x,long double h) {\n",
    "  return (function_fn(x+h) - function_fn(x))/h;\n",
    "}\n",
    "\n",
    "\n",
    "long double derivative_complex_step_approx(long double x,long  double h) {\n",
    "  return imag(function_cfn(dcomp(x,h)))/h;\n",
    "}\n",
    "\n",
    "\n",
    "long double derivative_ad_reverse(long double x) {\n",
    "    long double dy = 0;\n",
    "    auto f_grad = clad::gradient(function_fn, \"x\");\n",
    "    f_grad.execute(x, &dy);\n",
    "    return dy;\n",
    "}\n",
    "\n",
    "\n",
    "    \n",
    "auto fn_dx = clad::differentiate(function_fn, \"x\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaed4e1f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "    std::vector<float> x;\n",
    "    std::vector<float> y;\n",
    "    std::vector<float> y1;\n",
    "    double pi;\n",
    "    double a;\n",
    "    double b;\n",
    "    int N;\n",
    "    N = 1000;\n",
    "    pi =  2 * asin(1);\n",
    "    a = - pi/4;\n",
    "    b = pi / 2;\n",
    "    for (float j = a; j <= b; j += (b-a)/N) {\n",
    "        x.push_back(j);\n",
    "        y.push_back(function_fn(j));\n",
    "        y1.push_back(fn_dx.execute(j));\n",
    "    }\n",
    "\n",
    "plt::plot(x, y);\n",
    "plt::plot(x, y1);\n",
    "plt::ylim(0, 6);\n",
    "plt::xlim(-0.78, 1.7);\n",
    "plt::save(\"../../test/test3.png\");\n",
    "auto img = im::image(\"../../test/test3.png\");\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b87a39",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    std::vector<double> logspace;\n",
    "    std::vector<double> error_ff;\n",
    "    std::vector<double> error_complex_step;\n",
    "    std::vector<double> error_ad;\n",
    "double point_x;\n",
    "point_x =  pi/4;\n",
    "\n",
    "    for (int i = 15; i >= 1; i -= 1) {\n",
    "        logspace.push_back(pow(10,-i));\n",
    "    }\n",
    "\n",
    "for(double h : logspace) {\n",
    "   error_ff.push_back(abs(derivative_finite_diff_approx(point_x, h) - derivative_fn(point_x))/abs(derivative_fn(point_x)));\n",
    "   error_complex_step.push_back(abs(derivative_complex_step_approx(point_x, h) - derivative_fn(point_x))/abs(derivative_fn(point_x)));\n",
    "   //error_ad.push_back(abs(fn_dx.execute(point_x) - derivative_fn(point_x))/abs(derivative_fn(point_x)));   \n",
    "   error_ad.push_back(abs(derivative_ad_reverse(point_x) - derivative_fn(point_x))/abs(derivative_fn(point_x)));   \n",
    "}\n",
    "\n",
    "plt::figure();\n",
    "plt::loglog(logspace, error_ff);\n",
    "plt::loglog(logspace, error_complex_step, \"bo\");\n",
    "plt::loglog(logspace, error_ad);\n",
    "//plt::gca().invert_xaxis();\n",
    "//plt::xlim(pow(10,-18), 10);\n",
    "plt::save(\"../../test/test_err.png\");\n",
    "auto img2 = im::image(\"../../test/test_err.png\");\n",
    "img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d926d227",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "error_complex_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff724b3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "error_ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf252cc9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "auto f_grad = clad::gradient(function_fn, \"x\");\n",
    "f_grad.dump();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xeus-cling provides a Jupyter kernel for C++ with the help of the C++ interpreter cling and the native implementation of the Jupyter protocol xeus.\n",
    "\n",
    "Within the xeus-cling framework, Clad can enable automatic differentiation (AD) such that users can automatically generate C++ code for their computation of derivatives of their functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Mode AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a function _f_ of several inputs and single (scalar) output, forward mode AD can be used to compute (or, in case of Clad, create a function) computing a directional derivative of _f_ with respect to a single specified input variable. Moreover, the generated derivative function has the same signature as the original function _f_, however its return value is the value of the derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "double fn(double x, double y) {\n",
    "  return x*x*y + y*y;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto fn_dx = clad::differentiate(fn, \"x\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_dx.execute(5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse Mode AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse-mode AD enables the gradient computation within a single pass of the computation graph of _f_ using at most a constant factor (around 4) more arithmetical operations compared to the original function. While its constant factor and memory overhead is higher than that of the forward-mode, it is independent of the number of inputs.\n",
    "\n",
    "Moreover, the generated function has void return type and same input arguments. The function has an additional argument of type T*, where T is the return type of _f_. This is the “result” argument which has to point to the beginning of the vector where the gradient will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "double fn(double x, double y) {\n",
    "  return x*x + y*y;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto d_fn_2 = clad::gradient(fn, \"x, y\");\n",
    "double d_x, d_y;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code is: void fn_grad(double x, double y, clad::array_ref<double> _d_x, clad::array_ref<double> _d_y) {\n",
      "    double _t2;\n",
      "    double _t3;\n",
      "    double _t4;\n",
      "    double _t5;\n",
      "    _t3 = x;\n",
      "    _t2 = x;\n",
      "    _t5 = y;\n",
      "    _t4 = y;\n",
      "    double fn_return = _t3 * _t2 + _t5 * _t4;\n",
      "    goto _label0;\n",
      "  _label0:\n",
      "    {\n",
      "        double _r0 = 1 * _t2;\n",
      "        * _d_x += _r0;\n",
      "        double _r1 = _t3 * 1;\n",
      "        * _d_x += _r1;\n",
      "        double _r2 = 1 * _t4;\n",
      "        * _d_y += _r2;\n",
      "        double _r3 = _t5 * 1;\n",
      "        * _d_y += _r3;\n",
      "    }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_fn_2.dump();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "double kinetic_energy(double mass, double velocity) {\n",
    "  return mass * velocity * velocity * 0.5;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto hessian = clad::hessian(kinetic_energy, \"mass, velocity\");\n",
    "double matrix[4];\n",
    "hessian.execute(10, 2, matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ 0.0000000, 2.0000000, 2.0000000, 10.000000 }"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "void fn_jacobian(double i, double j, double *res) {\n",
    "  res[0] = i*i;\n",
    "  res[1] = j*j;\n",
    "  res[2] = i*j;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto d_fn = clad::jacobian(fn_jacobian);\n",
    "double res[3] = {0, 0, 0};\n",
    "double derivatives[6] = {0, 0, 0, 0, 0, 0};\n",
    "d_fn.execute(3, 5, res, derivatives);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ 6.0000000, 0.0000000, 0.0000000, 10.000000, 5.0000000, 3.0000000 }"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian matrix:\n",
      "6 0 \n",
      "0 10 \n",
      "5 3 \n"
     ]
    }
   ],
   "source": [
    "std::cout<<\"Jacobian matrix:\\n\";\n",
    "  for (int i=0; i<3; ++i) {\n",
    "    for (int j=0; j<2; ++j) {\n",
    "      std::cout<<derivatives[i*2 + j]<<\" \";\n",
    "    }\n",
    "    std::cout<<\"\\n\";\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Equation {\n",
    "  double m_x, m_y;\n",
    "\n",
    "  public:\n",
    "  Equation(double x, double y) : m_x(x), m_y(y) {}\n",
    "  double operator()(double i, double j) {\n",
    "    return m_x*i*j + m_y*i*j;\n",
    "  }\n",
    "  void setX(double x) {\n",
    "    m_x = x;\n",
    "  }\n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Equation E(3,5);\n",
    "auto d_E = clad::differentiate(E, \"i\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code is: double operator_call_darg0(double i, double j) {\n",
      "    double _d_i = 1;\n",
      "    double _d_j = 0;\n",
      "    double &_t2 = this->m_x;\n",
      "    double _t3 = _t2 * i;\n",
      "    double &_t4 = this->m_y;\n",
      "    double _t5 = _t4 * i;\n",
      "    return (0. * i + _t2 * _d_i) * j + _t3 * _d_j + (0. * i + _t4 * _d_i) * j + _t5 * _d_j;\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_E.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++11",
   "language": "C++11",
   "name": "xcpp11-clad"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Complex step approximation and Automatic Differentiation\n",
    "## Francesco Pasqualini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f260615",
   "metadata": {},
   "source": [
    "#include <string>\n",
    "#include <fstream>\n",
    "\n",
    "#include \"xtl/xbase64.hpp\"\n",
    "\n",
    "#include \"nlohmann/json.hpp\"\n",
    "#pragma cling add_include_path(\"/srv/conda/envs/notebook/include/python3.7m\")\n",
    "#pragma cling add_include_path(\"/srv/conda/envs/notebook/lib/python3.7/site-packages/numpy/core/include/\")\n",
    "#pragma cling add_library_path(\"/srv/conda/envs/notebook/lib/\")\n",
    "#pragma cling load(\"python3.7m\")\n",
    "#include \"clad/Differentiator/Differentiator.h\"\n",
    "#include <iostream>\n",
    "#include \"../../matplotlibcpp.h\"\n",
    "#include <iostream>\n",
    "#include <complex>\n",
    "#include <cmath>\n",
    "\n",
    "\n",
    "namespace plt = matplotlibcpp;\n",
    "namespace im\n",
    "{\n",
    "    struct image\n",
    "    {   \n",
    "        inline image(const std::string& filename)\n",
    "        {\n",
    "            std::ifstream fin(filename, std::ios::binary);   \n",
    "            m_buffer << fin.rdbuf();\n",
    "        }        \n",
    "        std::stringstream m_buffer;\n",
    "    };\n",
    "    nl::json mime_bundle_repr(const image& i)\n",
    "    {\n",
    "        auto bundle = nl::json::object();\n",
    "        bundle[\"image/png\"] = xtl::base64encode(i.m_buffer.str());\n",
    "        return bundle;\n",
    "    }\n",
    "}\n",
    "\n",
    "using namespace std;\n",
    "using namespace std::complex_literals;\n",
    "\n",
    "typedef complex<long double> dcomp;\n",
    "\n",
    "\n",
    "long double function_fn(long double x) {\n",
    "  return exp(x)/(pow((cos(x)),3) + pow(sin(x),3));\n",
    "}\n",
    "\n",
    "dcomp function_cfn(dcomp x) {\n",
    "  return exp(x)/(pow((cos(x)),3) + pow(sin(x),3));\n",
    "}\n",
    "\n",
    "\n",
    "double derivative_fn(double x) {\n",
    "  return (exp(x)*(cos(3*x) + sin(3*x)/2 + (3*sin(x))/2)) / \n",
    "            pow(pow(cos(x),3) + pow(sin(x),3),2);\n",
    "}\n",
    "\n",
    "long double derivative_finite_diff_approx(long double x,long double h) {\n",
    "  return (function_fn(x+h) - function_fn(x))/h;\n",
    "}\n",
    "\n",
    "\n",
    "long double derivative_complex_step_approx(long double x,long  double h) {\n",
    "  return imag(function_cfn(dcomp(x,h)))/h;\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "long double derivative_ad_reverse(long double x) {\n",
    "    long double dy = 0;\n",
    "    auto f_grad = clad::gradient(function_fn, \"x\");\n",
    "    f_grad.execute(x, &dy);\n",
    "    return dy;\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "//def dF_complex(x,h):\n",
    "//    '''Complex step aproximation'''\n",
    "//    #x =  np.array(x, dtype='cfloat')\n",
    "//    return np.imag(F(x + 1j*h))/h\n",
    "    \n",
    "    \n",
    "auto fn_dx = clad::differentiate(function_fn, \"x\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaed4e1f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "    std::vector<float> x;\n",
    "    std::vector<float> y;\n",
    "    std::vector<float> y1;\n",
    "    double pi;\n",
    "    double a;\n",
    "    double b;\n",
    "    int N;\n",
    "    N = 1000;\n",
    "    pi =  2 * asin(1);\n",
    "    a = - pi/4;\n",
    "    b = pi / 2;\n",
    "    for (float j = a; j <= b; j += (b-a)/N) {\n",
    "        x.push_back(j);\n",
    "        y.push_back(function_fn(j));\n",
    "        y1.push_back(fn_dx.execute(j));\n",
    "    }\n",
    "\n",
    "plt::plot(x, y);\n",
    "plt::plot(x, y1);\n",
    "plt::ylim(0, 6);\n",
    "plt::xlim(-0.78, 1.7);\n",
    "plt::save(\"../../test/test3.png\");\n",
    "auto img = im::image(\"../../test/test3.png\");\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b87a39",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "    std::vector<double> logspace;\n",
    "    std::vector<double> error_ff;\n",
    "    std::vector<double> error_complex_step;\n",
    "    std::vector<double> error_ad;\n",
    "double point_x;\n",
    "point_x =  pi/4;\n",
    "\n",
    "    for (int i = 15; i >= 1; i -= 1) {\n",
    "        logspace.push_back(pow(10,-i));\n",
    "    }\n",
    "\n",
    "for(double h : logspace) {\n",
    "   error_ff.push_back(abs(derivative_finite_diff_approx(point_x, h) - derivative_fn(point_x))/abs(derivative_fn(point_x)));\n",
    "   error_complex_step.push_back(abs(derivative_complex_step_approx(point_x, h) - derivative_fn(point_x))/abs(derivative_fn(point_x)));\n",
    "   //error_ad.push_back(abs(fn_dx.execute(point_x) - derivative_fn(point_x))/abs(derivative_fn(point_x)));   \n",
    "   error_ad.push_back(abs(derivative_ad_reverse(point_x) - derivative_fn(point_x))/abs(derivative_fn(point_x)));   \n",
    "}\n",
    "\n",
    "plt::figure();\n",
    "plt::loglog(logspace, error_ff);\n",
    "plt::loglog(logspace, error_complex_step, \"bo\");\n",
    "plt::loglog(logspace, error_ad);\n",
    "//plt::gca().invert_xaxis();\n",
    "//plt::xlim(pow(10,-18), 10);\n",
    "plt::save(\"../../test/test_err.png\");\n",
    "auto img2 = im::image(\"../../test/test_err.png\");\n",
    "img2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf252cc9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "auto f_grad = clad::gradient(function_fn, \"x\");\n",
    "f_grad.dump();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xeus-cling provides a Jupyter kernel for C++ with the help of the C++ interpreter cling and the native implementation of the Jupyter protocol xeus.\n",
    "\n",
    "Within the xeus-cling framework, Clad can enable automatic differentiation (AD) such that users can automatically generate C++ code for their computation of derivatives of their functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Mode AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a function _f_ of several inputs and single (scalar) output, forward mode AD can be used to compute (or, in case of Clad, create a function) computing a directional derivative of _f_ with respect to a single specified input variable. Moreover, the generated derivative function has the same signature as the original function _f_, however its return value is the value of the derivative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "double fn(double x, double y) {\n",
    "  return x*x*y + y*y;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto fn_dx = clad::differentiate(fn, \"x\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fn_dx.execute(5, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse Mode AD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reverse-mode AD enables the gradient computation within a single pass of the computation graph of _f_ using at most a constant factor (around 4) more arithmetical operations compared to the original function. While its constant factor and memory overhead is higher than that of the forward-mode, it is independent of the number of inputs.\n",
    "\n",
    "Moreover, the generated function has void return type and same input arguments. The function has an additional argument of type T*, where T is the return type of _f_. This is the “result” argument which has to point to the beginning of the vector where the gradient will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "double fn(double x, double y) {\n",
    "  return x*x + y*y;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto d_fn_2 = clad::gradient(fn, \"x, y\");\n",
    "double d_x, d_y;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code is: void fn_grad(double x, double y, clad::array_ref<double> _d_x, clad::array_ref<double> _d_y) {\n",
      "    double _t2;\n",
      "    double _t3;\n",
      "    double _t4;\n",
      "    double _t5;\n",
      "    _t3 = x;\n",
      "    _t2 = x;\n",
      "    _t5 = y;\n",
      "    _t4 = y;\n",
      "    double fn_return = _t3 * _t2 + _t5 * _t4;\n",
      "    goto _label0;\n",
      "  _label0:\n",
      "    {\n",
      "        double _r0 = 1 * _t2;\n",
      "        * _d_x += _r0;\n",
      "        double _r1 = _t3 * 1;\n",
      "        * _d_x += _r1;\n",
      "        double _r2 = 1 * _t4;\n",
      "        * _d_y += _r2;\n",
      "        double _r3 = _t5 * 1;\n",
      "        * _d_y += _r3;\n",
      "    }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_fn_2.dump();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "double kinetic_energy(double mass, double velocity) {\n",
    "  return mass * velocity * velocity * 0.5;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto hessian = clad::hessian(kinetic_energy, \"mass, velocity\");\n",
    "double matrix[4];\n",
    "hessian.execute(10, 2, matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ 0.0000000, 2.0000000, 2.0000000, 10.000000 }"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "void fn_jacobian(double i, double j, double *res) {\n",
    "  res[0] = i*i;\n",
    "  res[1] = j*j;\n",
    "  res[2] = i*j;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto d_fn = clad::jacobian(fn_jacobian);\n",
    "double res[3] = {0, 0, 0};\n",
    "double derivatives[6] = {0, 0, 0, 0, 0, 0};\n",
    "d_fn.execute(3, 5, res, derivatives);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ 6.0000000, 0.0000000, 0.0000000, 10.000000, 5.0000000, 3.0000000 }"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jacobian matrix:\n",
      "6 0 \n",
      "0 10 \n",
      "5 3 \n"
     ]
    }
   ],
   "source": [
    "std::cout<<\"Jacobian matrix:\\n\";\n",
    "  for (int i=0; i<3; ++i) {\n",
    "    for (int j=0; j<2; ++j) {\n",
    "      std::cout<<derivatives[i*2 + j]<<\" \";\n",
    "    }\n",
    "    std::cout<<\"\\n\";\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Equation {\n",
    "  double m_x, m_y;\n",
    "\n",
    "  public:\n",
    "  Equation(double x, double y) : m_x(x), m_y(y) {}\n",
    "  double operator()(double i, double j) {\n",
    "    return m_x*i*j + m_y*i*j;\n",
    "  }\n",
    "  void setX(double x) {\n",
    "    m_x = x;\n",
    "  }\n",
    "};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Equation E(3,5);\n",
    "auto d_E = clad::differentiate(E, \"i\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The code is: double operator_call_darg0(double i, double j) {\n",
      "    double _d_i = 1;\n",
      "    double _d_j = 0;\n",
      "    double &_t2 = this->m_x;\n",
      "    double _t3 = _t2 * i;\n",
      "    double &_t4 = this->m_y;\n",
      "    double _t5 = _t4 * i;\n",
      "    return (0. * i + _t2 * _d_i) * j + _t3 * _d_j + (0. * i + _t4 * _d_i) * j + _t5 * _d_j;\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d_E.dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "C++11",
   "language": "C++11",
   "name": "xcpp11-clad"
  },
  "language_info": {
   "codemirror_mode": "text/x-c++src",
   "file_extension": ".cpp",
   "mimetype": "text/x-c++src",
   "name": "c++",
   "version": "11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
